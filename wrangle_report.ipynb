{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT 2: TWITTER DATA WRANGLING\n",
    "\n",
    "#### Introduction: \n",
    "The project is to introduce the learner to the real world of messy data as data is real world is rarely clean. The analyst is usually needed to clean quality and tidiness issues of the data - a process known as **data wrangling** before it can be analysed. In this project, the real world data used are gotten from various sources in different formats as provided by a twitter user (@dog_rates) also known as WeRateDogs. The account rates people's dogs with humorous comments about the dogs with the ratings almost always having a denominator of 10 while the numerator is almost always greater than 10. The timestamp of the archive provided by WeRateDogs did not go beyond August 1, 2017.\n",
    "\n",
    "\n",
    "#### Project Motivation:\n",
    "The motivation behind the project is to wrangle the datasets effectively and produce beautiful, trustworthy and meaningful insights and visualizations.\n",
    "\n",
    "\n",
    "#### Data \n",
    "There are three datasets to be sourced and used. These Datasets are:\n",
    "\n",
    "1. **_Enhanced Twitter Archive_**: The file provided by *WeRateDogs* contains *2356 rows and 17 columns* of tweets about rated dogs. The rows show the number of tweets while the columns show other information about the tweets. The information include but not limited to reply, retweet, dog name, dog ratings, dog stages etc.\n",
    "\n",
    "\n",
    "2. **_Image Prediction File_**: This file is as a result of neural network developed by some students in Udacity and ran against the enhanced twitter archive provided by *WeRateDogs* to classify breeds of dog. The file contains *2075 rows and 12 columns* The rows show the number of predictions made while the columns show information about the prediction. The information include but not limited to image url, image prediction confidence, tweet ID etc.\n",
    "\n",
    "\n",
    "3. **_Twitter Json File_**: This file was scrapped from Twitter using the Tweepy API to extract the retweet and favourite counts about each tweet presented in *Enhanced Twitter Archive*. The file contains 2354 rows and 3 columns.\n",
    "\n",
    "\n",
    "#### Project Steps Overview\n",
    "The step followed in this project are:\n",
    "\n",
    "1. Gathering Data\n",
    "\n",
    "2. Assessing Data\n",
    "\n",
    "3. Cleaning Data\n",
    "\n",
    "4. Storing Data\n",
    "\n",
    "5. Analyzing and visualizing Data\n",
    "\n",
    "6. Reporting\n",
    "\n",
    "\n",
    "The tools used for this project are:\n",
    "\n",
    "1. Jupyter Notebooks\n",
    "\n",
    "2. Pandas Library\n",
    "\n",
    "3. Numpy Library\n",
    "\n",
    "4. Matplotlib Library\n",
    "\n",
    "5. Request Library\n",
    "\n",
    "6. OS Library\n",
    "\n",
    "7. Seaborn Library\n",
    "\n",
    "8. Json Library\n",
    "\n",
    "\n",
    "#### Gathering Data:\n",
    "1. **Twitter_Enhanced_Archive.csv**: This data was provided by Udacity and was manually downloaded and uploaded for use in the notebook. The uploaded file was read into a dataframe using the **_pd.read_csv('file_name')_** of the Pandas Library\n",
    "\n",
    "\n",
    "2. **image-prediction.tsv**: This data was hosted on Udacity server and programmatically downloaded for use in the notebook using **_get()_** function of the Request Library. After it was downloaded successfully, a response message of *200* was displayed and the **_with open()_** of the OS Library was used to save it into the notebook and **_pd.read_csv('file_name')__** of the Pandas Library was used to read it into a dataframe.\n",
    "\n",
    "\n",
    "3. **twitter-json.txt**: This data was supposed to be scrapped from Twitter but I could not do this as I do not have the elevated access to the Twitter Developer account as at the time of doing this project. So, the already scrapped json file provided by Udacity was manually downloaded and uploaded into the notebook. The tweet_id, retweet_count and favourite_count were extracted from the file using **_with open()_** of the OS Library, **_for loop_** of Pandas Library and loaded the lines from the file as **_json_**.\n",
    "\n",
    "\n",
    "#### Assessing Data\n",
    "The data was assessed in two ways namely:\n",
    "\n",
    "1. **Visual Assessment**: This was done by printing each dataframe and going through the datasets.\n",
    "\n",
    "2. **Programmatic Assessment**: This was done using the methods from the Pandas Library. The methods used are: _head(), tail(), sample(), describe(), info(), isnull(), isduplicated(), shape, value_counts()_\n",
    "\n",
    "\n",
    "#### Cleaning Data\n",
    "To clean the data, the '**_Define-Code-Test_** methodology was used. This methodology was used to clean the quality and tidiness issues found in the datasets. \n",
    "\n",
    "The following was done during the data cleansing stage:\n",
    "\n",
    "1. The three original dataframes were duplicated with the copy cleaned.\n",
    "\n",
    "2. I used the **_Define-Code-Test_** methodology to clean the following quality and tidiness issues.\n",
    "    \n",
    "    ##### Quality issues\n",
    "    1. The datatype of the column named '_timestamp_' in Twitter Archive Data (df1) is a '**string**' which is a wrong datatype for such column. The correct datatype should be '**datetime**'.\n",
    "\n",
    "    2. The datatype of the column named '*tweet_id*'in Twitter Archive Data (df1), Tweet Image Prediction Data (df2) and Twitter Json Data (df3) should be a **string** not an **integer** as seen.\n",
    "\n",
    "    3. There is are missing values in '*expanded_url*' column in Twitter Archive Data. \n",
    "\n",
    "    4. There are invalid values in the '_name_' column in Twitter Archive Data. The values in lowercase are '**articles**' not names of dogs\n",
    "\n",
    "    5. '**tweet_id**' with values in '**retweeted_status_id**', '**retweeted_status_user_id**', '**retweeted_status_timestamp**' in the Twitter Archive Data are not needed for this report\n",
    "\n",
    "    6. There are inconstitencies in the values in the columns '**p1**', '**p2**', '**p3**' in Tweet Image Prediction Data\n",
    "\n",
    "    7. The following columns: '**retweeted_status_id**', '**retweeted_status_user_id**', '**retweeted_status_timestamp**', '**in_reply_to_status_id**' and '**in_reply_to_status_user_id**' in the Twitter Archive Data consist of mainly missing values\n",
    "\n",
    "    8. The number of decimal places of the values in the columns '**p1_conf**', '**p2_conf**', '**p3_conf**' in Tweet Image Prediction Data es\n",
    "\n",
    "   ##### Tidiness Issues\n",
    "      1. Four columns in the Twitter Archive Data are supposed to be combined into one column with the column names as values in the new column. The affected columns are: '_doggo_', '_floofer_', '_pupper_' and '_puppo_'\n",
    "\n",
    "      2. The three tables should be merged into on Dataset\n",
    "\n",
    "\n",
    "#### Storing Data\n",
    "After succesfull executing the gathering, assessing and cleaning stages, the file was merged using the **pd.merge()** method of Pandas Library\n",
    "\n",
    "\n",
    "#### Analyzing and Visualization\n",
    "In this section, the merged file was loaded into the notebook and six insights were drawn and two visualization were made using the Pandas and Seaborn Libraries respectively.\n",
    "\n",
    "\n",
    "#### Reporting\n",
    "In this section, a report was written on the process using the notebooks provided. The notebook povided are: **wrangle_report.ipynb** and **act_report.ipynb**. The notebooks were then downloaded as **_html_** and **_pdf_** and uploaded into the workbook environment for submission.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
